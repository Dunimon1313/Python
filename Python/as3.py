# -*- coding: utf-8 -*-
"""AS3.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1x5Yx_QWrBA2S304ivDTOlGTxfSteGMfP

I chose to work with Gaussian Naive Bayes because not only was it the first formula I saw, it was also the most accessible to learn. As far as models go GNB trains fast and is really nice in that you just need to estimate the mean and standard deviation from the training set to make a prediction. [Source] [link text](https://machinelearningmastery.com/naive-bayes-for-machine-learning/#:~:text=Gaussian%20Naive%20Bayes&text=Other%20functions%20can%20be%20used,deviation%20from%20your%20training%20data.)
"""

import numpy as np
import pandas as pd
import matplotlib as mpl
import scipy
import matplotlib.pyplot as plt
from pylab import rcParams
import urllib
import sklearn
from sklearn.naive_bayes import GaussianNB
from sklearn import neighbors
from sklearn import preprocessing
from sklearn.model_selection import train_test_split
from sklearn.metrics import f1_score, accuracy_score, roc_auc_score, plot_roc_curve, classification_report, confusion_matrix
from sklearn.preprocessing import StandardScaler
from sklearn import metrics

"""Data Loading"""

dftrain = pd.read_csv("/content/drive/MyDrive/a3/train.csv").drop('id', axis = 1)
dftest = pd.read_csv("/content/drive/MyDrive/a3/test.csv")
dftrain

dftest

"""Nans check"""

dftrain.isna().sum().sum()

dftest.isna().sum().sum()

dftrain.columns

"""Nans, since there are none here, do not need to be handled

Outlier Check
"""

dftrain.loc[:, dftrain.columns[dftrain.max()<=1].to_list()].boxplot(figsize=(20,10), rot = 90);

dftrain.loc[:, dftrain.columns[dftrain.max()>1].to_list()].boxplot(figsize=(20,10), rot = 90);

"""Outlier Handling - removing and casting outliers to median had too strong of an effect on my model's accuracy, thus I will use corr coefs to find which columns to keep and train with."""

#for column in dftrain:
 # dftrain[column] = np.where(dftrain[column] > (dftrain[column].quantile(0.95)), (dftrain[column].quantile(0.50)), dftrain[column])

#dftrain.loc[:, dftrain.columns[dftrain.max()<=1].to_list()].boxplot(figsize=(20,10), rot = 90);

#dftrain.loc[:, dftrain.columns[dftrain.max()>1].to_list()].boxplot(figsize=(20,10), rot = 90);

"""Outliers are mostly fixed."""

fgl = dftrain["diagnosis"]
for column in dftrain:
  r = np.corrcoef(dftrain[column], fgl)
  print(column)
  print("R", r)

"""There are a lot of columns here, I am going to use the correlation coefficients of these columns against the diagnosis to see which whould be fed into the model. I am going to drop anything with a correlation less than .35

Drop: fractal_dimension_worst, symmetry_worst, fractal_dimension_se, symmetry_se, smoothness_se, texture_se, fractal_dimension_mean, smoothness_mean
"""

dftrain = dftrain.drop(["fractal_dimension_worst", "symmetry_mean", "fractal_dimension_se", "symmetry_se", "smoothness_se",
                        "texture_se", "fractal_dimension_mean", "smoothness_mean"], axis = "columns")
dftrain

#dftest = dftest.drop(["fractal_dimension_worst", "symmetry_mean", "fractal_dimension_se", "symmetry_se", "smoothness_se",
    #                    "texture_se", "fractal_dimension_mean", "smoothness_mean"], axis = "columns")
dftest

"""As far as feature scaling goes, graphical models such as naive bayes are supposed to be insensitive to feature transformations, so I will not perform them here.

Data preprocessing
"""

y = dftrain["diagnosis"]
x = dftrain.drop("diagnosis", axis = 1)

x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = .3)

"""Model"""

bay = GaussianNB()
bay.fit(x_train, y_train)
bay.score(x_test, y_test)

x_train.shape

y_train.shape

y_test.shape

x_test.shape

dftest.drop(columns='id', inplace=True)
dftest.shape

results = bay.predict(dftest)

output = pd.DataFrame({"diagnosis":results})
output.to_csv("as3sol.csv")
output