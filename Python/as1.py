# -*- coding: utf-8 -*-
"""AS1.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1ioCk7-61QDC9p9qnxmqL9oYNnF-lcP1G

Project Checklist :

You must load the data from the provided CSV files.

You must check for missing values within the training data.

If the training data contains missing values, you must describe and implement an approach to handle those missing values.

You must check for outliers within the training data.

If the training data contains outliers, you must describe and implement an approach to handle those outliers.

You must determine whether or not you will implement normalization or standardization, and explain your decision.

You must build and train a decision tree model on the training data.
You must report the best ROC AUC score, F1 score, and accuracy score that you were able to obtain for your decision tree model.

You must build and train a random forest model on the training data.
You must report the best ROC AUC score, F1 score, and accuracy score that you were able to obtain for your random forest model.

You must select the best model that you are able to generate and use that model to predict the target vector for the test data.

Your notebook must be saved with the output enabled so that we can see the results of each cell after it has been run.

Failure to adhere to this criterion will result in a 0 for this portion of your score.
"""

# Commented out IPython magic to ensure Python compatibility.
import numpy as np
import pandas as pd
from sklearn.pipeline import make_pipeline
from sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV, StratifiedKFold
from sklearn import tree
from sklearn.tree import DecisionTreeClassifier
import math
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import f1_score, accuracy_score, roc_auc_score, plot_roc_curve
import matplotlib.pyplot as plt
import seaborn as sns
# %matplotlib inline
plt.style.use('seaborn-whitegrid')

"""Data Loading"""

dftrain = pd.read_csv("/content/drive/MyDrive/a1/train.csv")
dftest = pd.read_csv("/content/drive/MyDrive/a1/test.csv")
submission = pd.read_csv("/content/drive/MyDrive/a1/sample_submission.csv")

dftrain

dftest

submission

"""Find missing values"""

dftrain.isna().sum().sum()

"""Find outliers"""

#IQR method is what I will use to find outliers, and it works by finding an upper and lower bound from the available data. Any data that falls 1.5 times the iqr range above the upper bound
# or below the lower bound is an outlier

lower_bound = dftrain.quantile(.25)
upper_bound = dftrain.quantile(.75)
range = upper_bound - lower_bound

"""upper bound"""

print(upper_bound)

"""lower bound"""

print(lower_bound)

"""range"""

print(range)

print((dftrain < (lower_bound - 1.5*range)) | (dftrain > (upper_bound + 1.5*range)))
#since this doesn't show up too well in colab, I used a bocplot below

"""Outlier handling  ->  Due to the sheer number of "outliers", I am hesitant to label them as such and correct them when they might actually be meaningful

Scattered Nature is more easily seen in a boxplot
"""

dftrain.loc[:, dftrain.columns[dftrain.max()<=1].to_list()].boxplot(figsize=(20,10), rot = 90);

""""Tree-based algorithms, on the other hand, are fairly insensitive to the scale of the features."  [Source](https://www.analyticsvidhya.com/blog/2020/04/feature-scaling-machine-learning-normalization-standardization/)

Since using feature scaling for tree models does not work well, I will use neither

Split Data
"""

Xftr = dftrain.drop(columns=["Bankrupt"], axis = 1)
Ysol = dftrain["Bankrupt"]

x_train, x_test, y_train, y_test = train_test_split(Xftr, Ysol, test_size = .35, random_state = 1)
w_train, w_test, z_train, z_test = train_test_split(Xftr, Ysol, test_size = .3, random_state = 1)

"""Decision Tree Model Building"""

dec_tree = DecisionTreeClassifier(criterion = "entropy", max_depth = 3, random_state = 1)
dec_tree.fit(X = x_train, y = y_train)
dec_tree_predictions = dec_tree.predict(x_test)

"""Decision tree Accuracy Testing"""

dec_tree_predictions.shape

print("F1: ", f1_score(y_test, dec_tree_predictions))

print("ROC: ", roc_auc_score(y_test, dec_tree.predict_proba(x_test)[:,1]))

print("Accuracy: ", accuracy_score(y_test, dec_tree_predictions))

"""Random Forest Model Building

"""

ran_for = RandomForestClassifier(n_estimators = 9999, max_depth = 50, criterion = "entropy", random_state = 1)
ran_for = ran_for.fit(X = w_train, y = z_train)
ran_for_predictions = ran_for.predict(w_test)

"""Random forest acc testing"""

print("F1: ", f1_score(z_test, ran_for_predictions))

print("ROC: ", roc_auc_score(z_test, dec_tree.predict_proba(w_test)[:,1]))

print("Accuracy: ", accuracy_score(z_test, ran_for_predictions))

"""Output both predictions to CSV"""

dec_tree_predictions = dec_tree.predict_proba(dftest)
ran_for_predictions = ran_for.predict_proba(dftest)

ran_for_predictions

dec_tree_predictions

dec_tree_pred = pd.DataFrame({ "Bankrupt" :dec_tree_predictions[:,1]})

dec_tree_pred.to_csv("decision_tree.csv")

ran_tree_pred = pd.DataFrame({"Bankrupt" :ran_for_predictions[:,1]})

ran_tree_pred.to_csv("random_forest.csv")